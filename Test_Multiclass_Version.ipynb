{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_Multiclass_Version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag4d3N3G47F9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2779429f-47bb-4153-aafd-373206c8838b"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Nov 13 17:33 .\n",
            "drwxr-xr-x 1 root root 4096 Nov 18 07:00 ..\n",
            "drwxr-xr-x 1 root root 4096 Nov 13 17:34 .config\n",
            "drwxr-xr-x 1 root root 4096 Nov 13 17:33 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It12pPfO5yOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ebe434-ea96-4cb8-a416-3e94a74b7435"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVfyepku5yfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288ba796-ac5b-425c-bfb3-a927ebc35205"
      },
      "source": [
        "cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4XFaEeg5ypi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd31c558-2bb9-40ce-d754-1c07a35d19a0"
      },
      "source": [
        "cd /content/gdrive/My Drive/Pianalytix Research Intern/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Pianalytix Research Intern\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkGRzxLn5yuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ae94a8e4-b7f9-42ac-a41d-a39307d1a7a8"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/Pianalytix Research Intern'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAltUFjk5y3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50b00cb-3830-40e2-f171-e9818bad0a3c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import seaborn as sns\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "stoplist = stopwords.words('english')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from sklearn import preprocessing\n",
        "from tensorflow import keras\n",
        "from textblob import TextBlob\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyiEF455y-d"
      },
      "source": [
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP6hDmtL5zE7"
      },
      "source": [
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# import timeit\n",
        "\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   print(\n",
        "#       '\\n\\nThis error most likely means that this notebook is not '\n",
        "#       'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "#       'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "#   raise SystemError('GPU device not found')\n",
        "\n",
        "# def cpu():\n",
        "#   with tf.device('/cpu:0'):\n",
        "#     random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "#     net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "#     return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "# def gpu():\n",
        "#   with tf.device('/device:GPU:0'):\n",
        "#     random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "#     net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "#     return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# cpu()\n",
        "# gpu()\n",
        "\n",
        "# # Run the op several times.\n",
        "# print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "#       '(batch x height x width x channel). Sum of ten runs.')\n",
        "# print('CPU (s):')\n",
        "# cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "# print(cpu_time)\n",
        "# print('GPU (s):')\n",
        "# gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "# print(gpu_time)\n",
        "# print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2IIO7Q95zL6"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nuGPUPz5zSg"
      },
      "source": [
        "path1 =\"/content/gdrive/My Drive/Pianalytix Research Intern/train.csv\"\n",
        "class DataFrame_Loader():\n",
        "\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        \n",
        "        pass\n",
        "        \n",
        "    def load_data_files(self,path1):\n",
        "        dftrain = pd.read_csv(path1)\n",
        "        return dftrain\n",
        "\n",
        "# DL = DataFrame_Loader()\n",
        "# df = DL.load_data_files(path1)\n",
        "# df = df[['comment_text']]\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcTPyxgG5zZN"
      },
      "source": [
        "class DataFrame_Preprocessor():\n",
        "\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        \n",
        "        print(\"Preprocessing Object Created\")\n",
        "        \n",
        "        \n",
        "    def clean_text(self,text):\n",
        "        '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "        and remove words containing numbers.'''\n",
        "        text = text.lower()\n",
        "        text = re.sub('\\[.*?\\]', '', text)\n",
        "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "        text = re.sub('<.*?>+', '', text)\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "        text = re.sub('\\n', '', text)\n",
        "        text = re.sub('\\w*\\d\\w*', '', text)\n",
        "        return text\n",
        "    \n",
        "    def decontracted(self,text):\n",
        "\n",
        "        text = re.sub(r\"won\\'t\", \"will not\", text)\n",
        "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "        text = re.sub(r\"\\'re\", \" are\", text)\n",
        "        text = re.sub(r\"\\'s\", \" is\", text)\n",
        "        text = re.sub(r\"\\'d\", \" would\", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "        text = re.sub(r\"\\'t\", \" not\", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "        text = re.sub(r\"\\'m\", \" am\", text)\n",
        "        return text\n",
        "    \n",
        "    def removeNumbers(self,text):\n",
        "        \"\"\" Removes integers \"\"\"\n",
        "        text = ''.join([i for i in text if not i.isdigit()])         \n",
        "        return text\n",
        "\n",
        "    def replaceMultiQuestionMark(self,text):\n",
        "        \"\"\" Replaces repetitions of question marks \"\"\"\n",
        "        text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
        "        text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
        "        text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
        "        return text\n",
        "    \n",
        "    def clean_contractions(self,text, mapping):\n",
        "        specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "        for s in specials:\n",
        "            text = text.replace(s, \"'\")\n",
        "        text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
        "        return text\n",
        "\n",
        "    def __replace(self,word, pos=None):\n",
        "        \n",
        "        \"\"\" Creates a set of all antonyms \n",
        "        for the word and if there is only \n",
        "        one antonym, it returns it \"\"\"\n",
        "        \n",
        "        antonyms = set()\n",
        "        for synset in wordnet.synsets(word, pos=pos):\n",
        "            for lemma in synset.lemmas():\n",
        "                for antonym in lemma.antonyms():\n",
        "                    antonyms.add(antonym.name())\n",
        "        if len(antonyms) == 1:\n",
        "            return antonyms.pop()\n",
        "        else:\n",
        "            return None\n",
        "        \n",
        "    def __replaceNegations(self,text):\n",
        "        \n",
        "        \"\"\" Finds \"not\" and antonym for \n",
        "        the next word and if found, replaces \n",
        "        not and the next word with the antonym \"\"\"\n",
        "        \n",
        "        i, l = 0, len(text)\n",
        "        words = []\n",
        "        while i < l:\n",
        "            word = text[i]\n",
        "            if word == 'not' and i+1 < l:\n",
        "                ant = self.__replace(text[i+1])\n",
        "                if ant:\n",
        "                    words.append(ant)\n",
        "                    i += 2\n",
        "                    continue\n",
        "            words.append(word)\n",
        "            i += 1\n",
        "        return words\n",
        "    \n",
        "    def tokenize1(self,text):\n",
        "        finalTokens = []\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tokens = self.__replaceNegations(tokens)\n",
        "        for w in tokens:\n",
        "            if (w not in stoplist):\n",
        "                finalTokens.append(w)\n",
        "        text = \" \".join(finalTokens)\n",
        "        return text\n",
        "    \n",
        "    def stem_words(self,text):\n",
        "        \n",
        "        stemmer = PorterStemmer()\n",
        "        return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "    def get_tweet_sentiment(self,tweet): \n",
        "      '''Utility function to classify sentiment of passed tweet using textblob's sentiment method'''\n",
        "      # create TextBlob object of passed tweet text \n",
        "      analysis = TextBlob(tweet) \n",
        "      \n",
        "      # set sentiment \n",
        "      if analysis.sentiment.polarity < 0:\n",
        "          return 'Toxic'\n",
        "      elif analysis.sentiment.polarity == 0:\n",
        "          return 'Neutral'\n",
        "      elif analysis.sentiment.polarity > 0:\n",
        "          return 'Positive'\n",
        "  \n",
        "\n",
        "    def fetch_sentiment_using_textblob(self,text):\n",
        "        \n",
        "        analysis = TextBlob(text)\n",
        "        return 'Toxic' if analysis.sentiment.polarity > 0 else 'Non_Toxic'\n",
        "    \n",
        "    \n",
        "class Preprocessor_Execution():\n",
        "\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        \n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def Execute_Preprocessor(self,text):\n",
        "        \n",
        "        try:\n",
        "            preprocess = DataFrame_Preprocessor()\n",
        "\n",
        "\n",
        "            text['comment_text'] = text['comment_text'].apply(lambda x: preprocess.clean_text(x))\n",
        "\n",
        "            text['comment_text'] = text['comment_text'].apply(lambda x: preprocess.decontracted(x))\n",
        "\n",
        "            text['comment_text'] = text['comment_text'].apply(lambda x: preprocess.removeNumbers(x))\n",
        "\n",
        "            text['comment_text'] = text['comment_text'].apply(lambda x: preprocess.replaceMultiQuestionMark(x))\n",
        "\n",
        "            text['comment_text'] = text['comment_text'].apply(lambda x: preprocess.clean_contractions(x, contraction_mapping))\n",
        "\n",
        "            text['comment_text'] = text['comment_text'].apply(lambda x: preprocess.tokenize1(x))\n",
        "\n",
        "            text['comment_text'] = text['comment_text'].apply(lambda x: preprocess.stem_words(x))\n",
        "\n",
        "            text['Toxicity'] = text['comment_text'].apply(lambda x: preprocess.get_tweet_sentiment((x)))\n",
        "\n",
        "            return text\n",
        "        except ValueError as ve:\n",
        "            raise(ValueError(\"Error in Pre-Procesing {}\".format(ve)))\n",
        "    \n",
        "# PE = Preprocessor_Execution()\n",
        "# df = PE.Execute_Preprocessor(df)\n",
        "# print(df['Toxicity'].value_counts())\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtdX3Fen5zfJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "class DataFrame_Train_test_split():\n",
        "\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        \n",
        "        print(\"Preprocessor object created\")\n",
        "        \n",
        "    def Split(self,text):\n",
        "        \n",
        "        x = text['comment_text']\n",
        "        \n",
        "        y = pd.get_dummies(text['Toxicity'])\n",
        "        print(y.value_counts())\n",
        "        \n",
        "        return train_test_split(x,y,test_size=0.20, random_state=0,stratify=y)\n",
        "\n",
        "# TS = DataFrame_Train_test_split()\n",
        "# X_train, X_test, y_train, y_test = TS.Split(df)\n",
        "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3Y0x8ad50jp"
      },
      "source": [
        "class Keras_Tokenizer():\n",
        "\n",
        "    \n",
        "    def __init__(self,max_features):\n",
        "        \n",
        "        self.max_features =600\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __label_encoding(self,y_train):\n",
        "        \"\"\"\n",
        "        Encode the given list of class labels\n",
        "        :y_train_enc: returns list of encoded classes\n",
        "        :labels: actual class labels\n",
        "        \"\"\"\n",
        "        lbl_enc = LabelEncoder()\n",
        "\n",
        "        y_train_enc = lbl_enc.fit_transform(y_train)\n",
        "        labels = lbl_enc.classes_\n",
        "\n",
        "        return y_train_enc, labels\n",
        "    \n",
        "    \n",
        "    \n",
        "    def __word_embedding(self,train, test, max_features, max_len=100):\n",
        "        \n",
        "        \n",
        "        try:\n",
        "            \"\"\" Keras Tokenizer class object \"\"\"\n",
        "            tokenizer = text.Tokenizer(num_words=max_features)\n",
        "            tokenizer.fit_on_texts(train)\n",
        "\n",
        "            train_data = tokenizer.texts_to_sequences(train)\n",
        "            test_data = tokenizer.texts_to_sequences(test)\n",
        "\n",
        "            \"\"\" Get the max_len \"\"\"\n",
        "            vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "            \"\"\" Padd the sequence based on the max-length \"\"\"\n",
        "            x_train = sequence.pad_sequences(train_data, maxlen=max_len, padding='post')\n",
        "            x_test = sequence.pad_sequences(test_data, maxlen=max_len, padding='post')\n",
        "            \"\"\" Return train, test and vocab size \"\"\"\n",
        "            return tokenizer, x_train, x_test, vocab_size\n",
        "        except ValueError as ve:\n",
        "            raise(ValueError(\"Error in word embedding {}\".format(ve)))\n",
        "            \n",
        "            \n",
        "    def preprocess(self,X_train, X_test):\n",
        "        \n",
        "        tokenizer,x_pad_train, x_pad_valid, vocab_size = self.__word_embedding(X_train, X_test, self.max_features)\n",
        "    \n",
        "        return tokenizer,x_pad_train, x_pad_valid, vocab_size\n",
        "\n",
        "# KT = Keras_Tokenizer(600)\n",
        "# tokenizer, x_pad_train, x_pad_valid, vocab_size = KT.preprocess(X_train, X_test)\n",
        "# x_pad_train.shape,x_pad_valid.shape,vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lc-kP0o7dAw"
      },
      "source": [
        "class Glove_Vectors_Loader():\n",
        "\n",
        "    \n",
        "    def __init__(self,FileNmae,Mode,encoding):\n",
        "        self.FileName = '/content/gdrive/My Drive/Research Internship/glove.6B.200d.txt'\n",
        "        self.Mode = 'r'\n",
        "        self.encoding = 'cp437'\n",
        "        \n",
        "        \n",
        "        \n",
        "    def load_Glove_Vectors(self):\n",
        "        embeddings_index={}\n",
        "        with open(self.FileName ,self.Mode ,encoding =self.encoding ) as f:\n",
        "            for line in tqdm(f):\n",
        "                values=line.split()\n",
        "                word=values[0]\n",
        "                vectors=np.asarray(values[1:],'float32')\n",
        "                embeddings_index[word]=vectors\n",
        "        f.close()\n",
        "        return embeddings_index\n",
        "    \n",
        "    def __sent2vec(self,s):\n",
        "        words = str(s).lower()\n",
        "        words = word_tokenize(words)\n",
        "        words = [w for w in words if not w in stoplist]\n",
        "        words = [w for w in words if w.isalpha()]\n",
        "        M = []\n",
        "        for w in words:\n",
        "            try:\n",
        "                M.append(embeddings_index[w])\n",
        "            except:\n",
        "                continue\n",
        "        M = np.array(M)\n",
        "        v = M.sum(axis=0)\n",
        "        if type(v) != np.ndarray:\n",
        "            return np.zeros(100)\n",
        "        return v / np.sqrt((v ** 2).sum())\n",
        "    \n",
        "    def Fit_Transform(self,X_train,X_test):\n",
        "        \n",
        "        xtrain_glove = np.array([self.__sent2vec(x) for x in tqdm(X_train)])\n",
        "        xtest_glove = np.array([self.__sent2vec(x) for x in tqdm(X_test)])\n",
        "        return xtrain_glove,xtest_glove\n",
        "    \n",
        "class Glove_Vectors_Execution():\n",
        "\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        \n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def Execute_Glove_Vectors(self,X_train,X_test):\n",
        "        \n",
        "\n",
        "        GL = Glove_Vectors_Loader('glove.6B.200d.txt','r','cp437')\n",
        "        \n",
        "        embeddings_index = GL.load_Glove_Vectors()\n",
        "        \n",
        "        xtrain_glove,xtest_glove  = (lambda x,y: GL.Fit_Transform(x,y))(X_train,X_test)\n",
        "        \n",
        "        return embeddings_index,xtrain_glove,xtest_glove\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgw4e36t5zlK"
      },
      "source": [
        "from keras.layers import LeakyReLU\n",
        "class RNN_Bidirectional_lstm_Build_Pack():\n",
        "\n",
        "    \n",
        "    def __init__(self,\n",
        "                 input_length,\n",
        "                 output_length,\n",
        "                 vocab_size,\n",
        "                 optimizer,\n",
        "                 loss,\n",
        "                 metrics,\n",
        "                 batch_size,\n",
        "                 epochs,\n",
        "                 verbose):\n",
        "        \n",
        "        self.input_length =100\n",
        "        self.output_length= 100\n",
        "        self.vocab_size = 18311\n",
        "        self.optimizer = 'adam'\n",
        "        self.loss = 'categorical_crossentropy'\n",
        "        self.metrics = ['acc']\n",
        "        self.batch_size = 256\n",
        "        self.epochs = 200\n",
        "        self.verbose = 1\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "    \n",
        "    def build_rnn(self,vocab_size,output_dim, input_dim):\n",
        "\n",
        "        model = Sequential([\n",
        "            keras.layers.Embedding(self.vocab_size,output_dim = self.output_length,\n",
        "                                  input_length = self.input_length),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Bidirectional(keras.layers.LSTM(256,return_sequences=True)),\n",
        "            keras.layers.GlobalMaxPool1D(),\n",
        "            keras.layers.Dense(225),\n",
        "            keras.layers.LeakyReLU(alpha=0.2),\n",
        "            keras.layers.Dropout(0.3),\n",
        "            keras.layers.Dense(150),\n",
        "            keras.layers.LeakyReLU(alpha=0.2),\n",
        "            keras.layers.Dropout(0.2),\n",
        "            keras.layers.Dense(95),\n",
        "            keras.layers.LeakyReLU(alpha=0.2),\n",
        "            keras.layers.Dropout(0.1),\n",
        "            keras.layers.Dense(64),\n",
        "            keras.layers.LeakyReLU(alpha=0.2),\n",
        "            keras.layers.Dropout(0.1),\n",
        "            keras.layers.Dense(34),\n",
        "            keras.layers.LeakyReLU(alpha=0.2),\n",
        "            keras.layers.Dropout(0.1),\n",
        "            keras.layers.Dense(32),\n",
        "            keras.layers.LeakyReLU(alpha=0.2),\n",
        "            keras.layers.Dense(output_dim, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "    \n",
        "    \n",
        "    def Compile_and_Fit(self,x_pad_train,y_train,x_pad_valid,y_test,rnn_model):\n",
        "        \n",
        "        try:\n",
        "    \n",
        "            rnn_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
        "\n",
        "\n",
        "            rnn_model.fit(x_pad_train, \n",
        "                                    y_train,\n",
        "                                    batch_size=self.batch_size,\n",
        "                                   epochs=self.epochs,\n",
        "                                   verbose= self.verbose)\n",
        "\n",
        "            score = rnn_model.evaluate(x_pad_valid, y_test, verbose=1)\n",
        "\n",
        "            print(\"Loss:%.3f Accuracy: %.3f\" % (score[0], score[1]))\n",
        "\n",
        "            return rnn_model\n",
        "        \n",
        "        except ValueError as Model_Error:\n",
        "            raise(ValueError(\"Model Compiling Error {}\".format(Model_Error)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbyeYY8s5zrN"
      },
      "source": [
        "# import sys\n",
        "# print(sys.getrecursionlimit())\n",
        "# sys.setrecursionlimit(20**6)\n",
        "# class End_Pipeline_Excecution():\n",
        "    \n",
        "#     def __init__(self):\n",
        "        \n",
        "        \n",
        "#         print(\"End_Pipeline_Excecution Object Created\")\n",
        "        \n",
        "        \n",
        "#     def Execute_Fulll_Pipeline(self):\n",
        "        \n",
        "#         try:\n",
        "#             load = DataFrame_Loader()\n",
        "#             PE = Preprocessor_Execution()\n",
        "#             TS = DataFrame_Train_test_split()\n",
        "#             KT = Keras_Tokenizer(6000)\n",
        "#             # GVE  = Glove_Vectors_Execution()\n",
        "#             Rnn_Model = RNN_Bidirectional_lstm_Build_Pack(100,100,183111,'adam','binary_crossentropy',['acc'],256,5,1)\n",
        "#             df = load.load_data_files(path1)\n",
        "#             df = df[['comment_text']]\n",
        "#             print(\"DataFrame Shape\",df.shape)\n",
        "#             df = PE.Execute_Preprocessor(df)\n",
        "#             print(df['Toxicity'].value_counts())\n",
        "#             X_train, X_test, y_train, y_test = TS.Split(df)\n",
        "#             print(\"After Train Test Split\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "#             tokenizer, x_pad_train, x_pad_valid, vocab_size = KT.preprocess(X_train, X_test)\n",
        "#             print(\"After Tokenization\",tokenizer, x_pad_train.shape, x_pad_valid.shape, vocab_size)\n",
        "#             # embeddings_index,xtrain_glove,xtest_glove  = GVE.Execute_Glove_Vectors(x_pad_train,x_pad_valid)\n",
        "#             # print(\"After Glove Embedding\",len(embeddings_index),xtrain_glove.shape,xtest_glove.shape)\n",
        "#             rnn_model = Rnn_Model.build_rnn(vocab_size,3,100)\n",
        "#             print(rnn_model.summary())\n",
        "#             rnn_model = Rnn_Model.Compile_and_Fit(x_pad_train,y_train,x_pad_valid,y_test,rnn_model)\n",
        "#             return tokenizer,rnn_model,x_pad_valid,y_test\n",
        "#         except ValueError as ve:\n",
        "#             raise(ValueError(\"Error in Excecution {}\".format(ve)))\n",
        "        \n",
        "# End = End_Pipeline_Excecution()\n",
        "# tokenizer,rnn_model,x_pad_valid,y_test = End.Execute_Fulll_Pipeline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd0MQgpGOpQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21aec294-14f4-47c3-fb56-bbd49507412e"
      },
      "source": [
        "import sys\n",
        "print(sys.getrecursionlimit())\n",
        "sys.setrecursionlimit(20**6)\n",
        "class End_Pipeline_Excecution():\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        \n",
        "        print(\"End_Pipeline_Excecution Object Created\")\n",
        "        \n",
        "        \n",
        "    def Execute_Fulll_Pipeline(self):\n",
        "        \n",
        "        try:\n",
        "            load = DataFrame_Loader()\n",
        "            PE = Preprocessor_Execution()\n",
        "            TS = DataFrame_Train_test_split()\n",
        "            KT = Keras_Tokenizer(600)\n",
        "            # GVE  = Glove_Vectors_Execution()\n",
        "            Rnn_Model = RNN_Bidirectional_lstm_Build_Pack(100,100,18311,'adam','binary_crossentropy',['acc'],256,5,1)\n",
        "            df = load.load_data_files(path1)\n",
        "            df = df[['comment_text']]\n",
        "            print(\"DataFrame Shape\",df.shape)\n",
        "            df = PE.Execute_Preprocessor(df)\n",
        "            print(df['Toxicity'].value_counts())\n",
        "            X_train, X_test, y_train, y_test = TS.Split(df)\n",
        "            print(\"After Train Test Split\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "            tokenizer, x_pad_train, x_pad_valid, vocab_size = KT.preprocess(X_train, X_test)\n",
        "            print(\"After Tokenization\",tokenizer, x_pad_train.shape, x_pad_valid.shape, vocab_size)\n",
        "            # embeddings_index,xtrain_glove,xtest_glove  = GVE.Execute_Glove_Vectors(x_pad_train,x_pad_valid)\n",
        "            # print(\"After Glove Embedding\",len(embeddings_index),xtrain_glove.shape,xtest_glove.shape)\n",
        "            rnn_model = Rnn_Model.build_rnn(18311,3,100)\n",
        "            print(rnn_model.summary())\n",
        "            rnn_model = Rnn_Model.Compile_and_Fit(x_pad_train,y_train,x_pad_valid,y_test,rnn_model)\n",
        "            return tokenizer,rnn_model,x_pad_valid,y_test\n",
        "        except ValueError as ve:\n",
        "            raise(ValueError(\"Error in Excecution {}\".format(ve)))\n",
        "        \n",
        "End = End_Pipeline_Excecution()\n",
        "tokenizer,rnn_model,x_pad_valid,y_test = End.Execute_Fulll_Pipeline()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64000000\n",
            "End_Pipeline_Excecution Object Created\n",
            "Preprocessor object created\n",
            "DataFrame Shape (159571, 1)\n",
            "Preprocessing Object Created\n",
            "Positive    62157\n",
            "Neutral     60443\n",
            "Toxic       36971\n",
            "Name: Toxicity, dtype: int64\n",
            "Neutral  Positive  Toxic\n",
            "0        1         0        62157\n",
            "1        0         0        60443\n",
            "0        0         1        36971\n",
            "dtype: int64\n",
            "After Train Test Split (127656,) (31915,) (127656, 3) (31915, 3)\n",
            "After Tokenization <keras_preprocessing.text.Tokenizer object at 0x7ff2129a9208> (127656, 100) (31915, 100) 189183\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          1831100   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 100, 100)          400       \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 512)          731136    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 225)               115425    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 225)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 225)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 150)               33900     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 95)                14345     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 95)                0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 95)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                6144      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 34)                2210      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 32)                1120      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 2,735,879\n",
            "Trainable params: 2,735,679\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "499/499 [==============================] - 1419s 3s/step - loss: 0.4988 - acc: 0.8235\n",
            "Epoch 2/200\n",
            "499/499 [==============================] - 1410s 3s/step - loss: 0.4162 - acc: 0.8643\n",
            "Epoch 3/200\n",
            "499/499 [==============================] - 1401s 3s/step - loss: 0.4057 - acc: 0.8665\n",
            "Epoch 4/200\n",
            "499/499 [==============================] - 1414s 3s/step - loss: 0.3977 - acc: 0.8684\n",
            "Epoch 5/200\n",
            "499/499 [==============================] - 1412s 3s/step - loss: 0.3885 - acc: 0.8702\n",
            "Epoch 6/200\n",
            "499/499 [==============================] - 1448s 3s/step - loss: 0.3773 - acc: 0.8728\n",
            "Epoch 7/200\n",
            "499/499 [==============================] - 1468s 3s/step - loss: 0.3637 - acc: 0.8758\n",
            "Epoch 8/200\n",
            "499/499 [==============================] - 1426s 3s/step - loss: 0.3468 - acc: 0.8799\n",
            "Epoch 9/200\n",
            "499/499 [==============================] - 1440s 3s/step - loss: 0.3238 - acc: 0.8859\n",
            "Epoch 10/200\n",
            "499/499 [==============================] - 1571s 3s/step - loss: 0.2979 - acc: 0.8941\n",
            "Epoch 11/200\n",
            "499/499 [==============================] - 1562s 3s/step - loss: 0.2710 - acc: 0.9029\n",
            "Epoch 12/200\n",
            "499/499 [==============================] - 1569s 3s/step - loss: 0.2386 - acc: 0.9139\n",
            "Epoch 13/200\n",
            "499/499 [==============================] - 1555s 3s/step - loss: 0.2095 - acc: 0.9257\n",
            "Epoch 14/200\n",
            "499/499 [==============================] - 1560s 3s/step - loss: 0.1827 - acc: 0.9363\n",
            "Epoch 15/200\n",
            "499/499 [==============================] - 1573s 3s/step - loss: 0.1628 - acc: 0.9432\n",
            "Epoch 16/200\n",
            "499/499 [==============================] - 1581s 3s/step - loss: 0.1424 - acc: 0.9511\n",
            "Epoch 17/200\n",
            "499/499 [==============================] - 1563s 3s/step - loss: 0.1283 - acc: 0.9569\n",
            "Epoch 18/200\n",
            "499/499 [==============================] - 1558s 3s/step - loss: 0.1160 - acc: 0.9616\n",
            "Epoch 19/200\n",
            "499/499 [==============================] - 1562s 3s/step - loss: 0.1071 - acc: 0.9648\n",
            "Epoch 20/200\n",
            "499/499 [==============================] - 1536s 3s/step - loss: 0.0974 - acc: 0.9682\n",
            "Epoch 21/200\n",
            "499/499 [==============================] - 1548s 3s/step - loss: 0.0924 - acc: 0.9699\n",
            "Epoch 22/200\n",
            "499/499 [==============================] - 1546s 3s/step - loss: 0.0877 - acc: 0.9717\n",
            "Epoch 23/200\n",
            " 32/499 [>.............................] - ETA: 23:32 - loss: 0.0689 - acc: 0.9785"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-_mp89KnBnz"
      },
      "source": [
        "y_preds = rnn_model.predict(x_pad_valid)\n",
        "\n",
        "y_preds.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbGLLoQ7Fcx8"
      },
      "source": [
        "y_preds = pd.DataFrame(y_preds)\n",
        "y_preds.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i17VFpIGalso"
      },
      "source": [
        "y_preds.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D464UyuaFdLx"
      },
      "source": [
        "Predicted =y_preds.idxmax(axis=1)\n",
        "print(Predicted.value_counts())\n",
        "Predicted.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzIpWKWOFddj"
      },
      "source": [
        "y_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kA1x_Qn5zxH"
      },
      "source": [
        "x = y_test.stack()\n",
        "y_testt = pd.Series(pd.Categorical(x[x!=0].index.get_level_values(1)))\n",
        "y_testt.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYzznNmiHapQ"
      },
      "source": [
        "y_testt = pd.DataFrame(y_testt)\n",
        "y_testt.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsHNk1PDa9Sk"
      },
      "source": [
        "y_testt.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmZrq6HQ5z3e"
      },
      "source": [
        "# x = y_test.stack()\n",
        "# y_testt = pd.Series(pd.Categorical(x[x!=0].index.get_level_values(1)))\n",
        "# y_testt = pd.DataFrame(y_testt)\n",
        "def encode_ytest(ytest):\n",
        "  if ytest ==\"Neutral\":\n",
        "    return 0\n",
        "  elif ytest ==\"Positive\":\n",
        "    return 1\n",
        "  elif ytest == \"Toxic\":\n",
        "    return 2\n",
        "y_testt[0] = y_testt[0].apply(encode_ytest)\n",
        "print(y_testt.value_counts())\n",
        "y_testt.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAzsqaun5z9u"
      },
      "source": [
        "y_testt.shape,Predicted.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl81q-6t50Ds"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(metrics.accuracy_score(y_testt, Predicted))\n",
        "        \n",
        "print(metrics.confusion_matrix(y_testt, Predicted))\n",
        "        \n",
        "print(metrics.classification_report(y_testt, Predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flnL08MO50Js"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zKfJOno50QD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}